[05/15 22:52:26] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/15 22:52:26] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 22:52:26] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/15 22:52:26] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/15 22:52:26] d2.evaluation.evaluator INFO: Start inference on 125 batches
[05/15 22:55:43] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/15 22:55:43] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 22:55:43] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/15 22:55:43] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/15 22:55:44] d2.evaluation.evaluator INFO: Start inference on 125 batches
[05/15 22:57:50] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/15 22:57:50] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 22:57:50] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/15 22:57:50] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/15 22:57:51] d2.evaluation.evaluator INFO: Start inference on 500 batches
[05/15 22:58:15] d2.evaluation.evaluator INFO: Inference done 1/500. Dataloading: 0.2244 s/iter. Inference: 20.9744 s/iter. Eval: 2.8884 s/iter. Total: 24.0873 s/iter. ETA=3:20:19
[05/15 22:58:37] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/15 22:58:37] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 22:58:37] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/15 22:58:37] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/15 22:58:38] d2.evaluation.evaluator INFO: Start inference on 500 batches
[05/15 23:00:22] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/15 23:00:22] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 23:00:22] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/15 23:00:22] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/15 23:00:22] d2.evaluation.evaluator INFO: Start inference on 500 batches
[05/15 23:01:18] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/15 23:01:18] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 23:01:18] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/15 23:01:18] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/15 23:01:18] d2.evaluation.evaluator INFO: Start inference on 500 batches
[05/15 23:02:55] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/15 23:02:55] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 23:02:55] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/15 23:02:55] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/15 23:02:56] d2.evaluation.evaluator INFO: Start inference on 500 batches
[05/15 23:04:42] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/15 23:04:42] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 23:04:42] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/15 23:04:42] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/15 23:04:43] d2.evaluation.evaluator INFO: Start inference on 500 batches
[05/15 23:43:34] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/15 23:43:34] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 23:43:34] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/15 23:43:34] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/15 23:43:35] d2.evaluation.evaluator INFO: Start inference on 125 batches
[05/15 23:47:30] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/15 23:47:30] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 23:47:30] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/15 23:47:30] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/15 23:47:31] d2.evaluation.evaluator INFO: Start inference on 500 batches
[05/15 23:49:43] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/15 23:49:43] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/15 23:49:43] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/15 23:49:43] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/15 23:49:44] d2.evaluation.evaluator INFO: Start inference on 500 batches
[05/16 00:33:43] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]
[05/16 00:33:43] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[05/16 00:33:43] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[05/16 00:33:43] d2.data.common INFO: Serialized dataset takes 0.76 MiB
[05/16 00:33:44] d2.evaluation.evaluator INFO: Start inference on 500 batches
